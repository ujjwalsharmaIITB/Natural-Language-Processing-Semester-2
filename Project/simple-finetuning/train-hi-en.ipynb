{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HF_HOME'] = \"./hf/\"\n",
    "\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBartTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import BitsAndBytesConfig\n",
    "import numpy as np\n",
    "# import peft and Lora\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = LoraConfig(\n",
    "    r=20, #Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense'\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MBart50Tokenizer'. \n",
      "The class this function is called from is 'MBartTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer = MBartTokenizer.from_pretrained(model_name , use_fast = True)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the Model for QLoRA\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft model\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cfilt/iitb-english-hindi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "def generate_dataset(dataset , split):\n",
    "    filtered_dataset = dataset[split]['translation']\n",
    "    english_dataset = [data['en'] for data in filtered_dataset]\n",
    "    hindi_dataset = [data['hi'] for data in filtered_dataset]\n",
    "    dataset_size = min(30000 , len(english_dataset))\n",
    "\n",
    "\n",
    "    print(\"Total Dataset length : \" , len(english_dataset))\n",
    "    print(\"Trimmed length :\" , dataset_size)\n",
    "\n",
    "\n",
    "    english_dataset = english_dataset[:dataset_size]\n",
    "    hindi_dataset = hindi_dataset[:dataset_size]\n",
    "    data_dictionary = {\n",
    "        \"english\" : english_dataset,\n",
    "        \"hindi\" : hindi_dataset\n",
    "    }\n",
    "    return Dataset.from_dict(data_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dataset length :  1659083\n",
      "Trimmed length : 30000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['english', 'hindi'],\n",
       "    num_rows: 30000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = generate_dataset(dataset, \"train\")\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dataset length :  2507\n",
      "Trimmed length : 2507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['english', 'hindi'],\n",
       "    num_rows: 2507\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = generate_dataset(dataset , \"test\")\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dataset length :  520\n",
      "Trimmed length : 520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['english', 'hindi'],\n",
       "    num_rows: 520\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset = generate_dataset(dataset , \"validation\")\n",
    "validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_example(example , lang):\n",
    "#     return tokenizer(example[lang], truncation=True)\n",
    "\n",
    "# def tokenize_dataset(example):\n",
    "#     english_tokens = tokenize_example(example, \"english\")\n",
    "#     # english_tokens['english_tokens'] = english_tokens['input_ids']\n",
    "#     english_tokens['english_attention_mask'] = english_tokens['attention_mask']\n",
    "#     hindi_tokens = tokenize_example(example , \"hindi\")\n",
    "#     english_tokens['labels'] = hindi_tokens['input_ids']\n",
    "#     english_tokens['hindi_attention_mask'] = hindi_tokens['attention_mask']\n",
    "#     return english_tokens\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_dataset_new(example):\n",
    "    model_inputs = tokenizer(example[\"hindi\"], max_length=512, truncation=True)\n",
    "    labels = tokenizer(example[\"english\"], max_length=512, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa8afaed61b4283917c3bd1cdfc84ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=5):   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 30000\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenised_dataset = train_dataset.map(tokenize_dataset_new , batched=True , num_proc=5)\n",
    "train_tokenised_dataset = train_tokenised_dataset.remove_columns(['english' , 'hindi'])\n",
    "train_tokenised_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12cae1f992a4c0bb81ce38c184feb3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=5):   0%|          | 0/2507 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 2507\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokenised_dataset = test_dataset.map(tokenize_dataset_new , batched=True , num_proc=5)\n",
    "test_tokenised_dataset = test_tokenised_dataset.remove_columns(['english' , 'hindi' ])\n",
    "test_tokenised_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a114fa6298043f194cd989407ca5123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=5):   0%|          | 0/520 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 520\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_tokenised_dataset = validation_dataset.map(tokenize_dataset_new , batched=True , num_proc=5)\n",
    "validation_tokenised_dataset = validation_tokenised_dataset.remove_columns(['english' , 'hindi'])\n",
    "validation_tokenised_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collector = DataCollatorForSeq2Seq(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "train_batch_size = 30\n",
    "test_batch_size = 15\n",
    "validation_batch_size = 10\n",
    "# train_dataloader = DataLoader(train_tokenised_dataset , shuffle=True,\n",
    "#                                 batch_size = train_batch_size,\n",
    "#                                 collate_fn = data_collector\n",
    "#                                 )\n",
    "\n",
    "# test_dataloader = DataLoader(test_tokenised_dataset , shuffle=True,\n",
    "#                                 batch_size = test_batch_size,\n",
    "#                                 collate_fn = data_collector\n",
    "#                                 )\n",
    "\n",
    "\n",
    "# validation_dataloader = DataLoader(validation_tokenised_dataset , shuffle=True,\n",
    "#                                 batch_size = validation_batch_size,\n",
    "#                                 collate_fn = data_collector\n",
    "#                                 )\n",
    "\n",
    "# train_dataloader,test_dataloader, validation_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    # training\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    # per device training batch size is used to train the model on the given batch size\n",
    "\n",
    "    # evaluation\n",
    "    per_device_eval_batch_size=validation_batch_size,\n",
    "    # per device evaluation batch size is used to evaluate the model on the given batch size\n",
    "    # gradient_accumulation_steps=8,\n",
    "    # gradient accumulation steps is used to accumulate the gradients over the given number of steps\n",
    "    # this helps in reducing the memory usage during training\n",
    "    # eval_accumulation_steps=10,\n",
    "    # eval accumulation steps is used to accumulate the evaluation results over the given number of steps\n",
    "    # this helps in reducing the memory usage during evaluation\n",
    "    evaluation_strategy=\"steps\",\n",
    "    # if the evaluation strategy is steps, then the evaluation will be done every eval_steps\n",
    "    # else if it is epoch, then the evaluation will be done every epoch and eval accumulation steps will be ignored\n",
    "    eval_steps=500,\n",
    "\n",
    "\n",
    "    # checkpointing\n",
    "\n",
    "\n",
    "    # logging\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "\n",
    "    # misc\n",
    "    warmup_steps=500,\n",
    "    # warmup steps is used to warmup the learning rate over the given number of steps\n",
    "    # this helps in reducing the impact of the randomness in the initial learning rate\n",
    "    # this is very useful when the learning rate is very high\n",
    "    # this is also useful when the model is very large\n",
    "    output_dir=\"./output\",\n",
    "    save_steps=500,\n",
    "    # save steps is used to save the model over the given number of steps\n",
    "    # this is useful when the model is very large\n",
    "    save_strategy=\"steps\",\n",
    "    # save strategy is used to save the model every epoch\n",
    "    # if the save strategy is steps, then the model will be saved every save_steps\n",
    "    # else if it is epoch, then the model will be saved every epoch\n",
    "    # and save_steps will be ignored\n",
    "    save_total_limit=4,\n",
    "\n",
    "    # save the best model\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better = False,\n",
    "    # generate tensorboard logs\n",
    "    report_to=None,\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/nlp/ssmt/anaconda3/envs/hugging-face/lib/python3.12/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenised_dataset,\n",
    "    eval_dataset=validation_tokenised_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks = [early_stopping]\n",
    "    # compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3500' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3500/5000 12:25 < 05:19, 4.69 it/s, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.352400</td>\n",
       "      <td>1.552265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.333700</td>\n",
       "      <td>1.540799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.095600</td>\n",
       "      <td>1.554042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.050200</td>\n",
       "      <td>1.564358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.992100</td>\n",
       "      <td>1.574079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.018100</td>\n",
       "      <td>1.576604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.858200</td>\n",
       "      <td>1.588758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3500, training_loss=1.2506326999664306, metrics={'train_runtime': 746.5867, 'train_samples_per_second': 200.914, 'train_steps_per_second': 6.697, 'total_flos': 8527782800424960.0, 'train_loss': 1.2506326999664306, 'epoch': 3.5})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./mBART-fine-tuned-hi-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for translation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation\", model=model,\n",
    "                    src_lang = \"hi_IN\" , tgt_lang= \"en_XX\",\n",
    "                    tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'How are you'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator(\"आप कैसे हैं\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging-face",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
